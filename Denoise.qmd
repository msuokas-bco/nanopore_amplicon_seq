---
title: "Denoising nanopore reads"
format: pdf
editor: visual
---

```{r font_size, include=F}
# This will allow to use different font sizes inside code chunks
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
# Suppress warnings and messages globally
knitr::opts_chunk$set(warning=F, message=F)
```

#### Preprocessing Reads

Dorado does not support demultiplexing dual indexes on both the 5' and 3' ends. Additionally, in ligated libraries, reads can occur in either orientation. To address this, we use `cutadapt` for demultiplexing. Index pairs are identified using the linked adapters approach in both forward and reverse orientations, followed by scripts that reverse complement the reverse reads. Finally, the reads are merged.

**Note:** Autocorrect may alter double dashes in command-line examples, so ensure they are correctly formatted.

#### Extracting Forward Reads

To extract forward reads into a FASTQ file, use the following command:

``` bash
cutadapt -e 0 -O 12 -g file:~/scripts/barcodes.fasta --trimmed-only \
-m 1200 -o "fdemuxed/{name}.fastq.gz" reads.fastq.gz
```

This command extracts barcodes specified in the `barcodes.fasta` file and outputs the matched reads into individual files within the `fdemuxed` subdirectory. In this example, the minimum read length is set to 1200 bp.

#### Extracting Reverse Reads

For reverse reads, use the reverse-complemented barcode file:

``` bash
cutadapt -e 0 -O 12 -g file:~/scripts/rev_barcodes.fasta --trimmed-only \
-m 1200 -o "rdemuxed/{name}.fastq.gz" reads.fastq.gz
```

The reverse reads are demultiplexed into the `rdemuxed` directory.

**Tip:** You can use parameters `-O`, `-e`, `-m`, and `-M` to reduce the chances of mismatched alignments.

#### Reverse Complementing Reverse Reads

Next, we use a bash script to reverse complement each reverse read file with the following command:

``` bash
seqkit seq -rp --seq-type DNA -o reverse_comp.fastq.gz reverse_out.fastq.gz
```

#### Merging Forward and Reverse Reads

Finally, you can merge forward and reverse reads with the same base name from two directories using a simple bash script:

``` bash
zcat forward_out.fastq.gz reverse_comp.fastq.gz > merged_reads.fastq.gz
```

#### Trimming Primers

Once the reads are merged, `cutadapt` and bash scripts can be used to trim forward and reverse PCR primers from the sequence reads.

\newpage

#### Import set2

Load libraries

```{r libraries, size="tiny"}
library(dada2)
library(knitr)
library(Biostrings)
library(tidyverse)
library(kableExtra)
library(mia)
library(ape)
```

Set variables

```{r variables, size="tiny"}
# Path variables
path <- "data/reads/set2"
silva <- "~/feature_classifiers/silva_nr99_v138.1_train_set.fa.gz"
species <- "~/feature_classifiers/silva_species_assignment_v138.1.fa.gz"
meta_file <- "data/set2_meta.tsv"
exportloc <- "results/denoised/"
#Creates results directory
dir.create(exportloc)
#metadata
metadata <- data.frame(read_tsv(meta_file, show_col_types = F))
metadata <- column_to_rownames(metadata, "Sampleid")
```

\newpage

For the project, we took advantage of computing power of CSC and imported already executed data objects. R code is unaltered. Execution is controlled by eval parameter in code chunk. RDS files also save resources and time when document is edited and checked.

```{r, size="tiny"}
#List files inside directory
list.files(path)
# Forward fastq filenames have format: SAMPLENAME_R1_001.fastq
fnFs <- sort(list.files(path, pattern=".fastq.gz", full.names = T))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

#### Filter sequence data

Filtering reads. Approximate 1 error for each 200 bp should be good starting point. There is no reason to truncate reads as sequence quality doesn't seem to decrease as a function of read length.

```{r trimfilter, size = "tiny", eval = F}
# Filtered files are placed in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names,
                                             "_F_filt.fastq.gz"))
# For single end data sets without phix control
names(filtFs) <- sample.names
out <- filterAndTrim(fnFs, filtFs, truncLen=0,
                     maxN = 0, maxEE = 7 , truncQ = 2,
                     compress = T, multithread = F,
                     rm.phix = F)
saveRDS(out, "data/denoised/out.rds")
```

```{r, size="tiny"}
out <- readRDS("data/denoised/out.rds")
```

#### Learn error rates

```{r error_rates, size="tiny", eval=F}
# Forward read error rate
errF <- learnErrors(filtFs, multithread=2)
saveRDS(errF, "data/denoised/errF.rds")
```

#### Plot error profile

```{r error_profile, size="tiny"}
errF <- readRDS("data/denoised/errF.rds")
plotErrors(errF, nominalQ=T)
```

#### Denoise sequences

Denoising parameters are adjusted to same values as has been recommended to pyrosequence type of data.

```{r denoise, size="tiny", eval=F}
setDadaOpt(HOMOPOLYMER_GAP_PENALTY=-1, BAND_SIZE=32)
dadaFs <- dada(FiltFs, multithread=2, verbose=T)
saveRDS(dadaFs,"data/denoised/dadaFs.rds")
```

```{r, size="tiny"}
dadaFs <- readRDS("data/denoised/dadaFs.rds")
```

#### Create ASV table

```{r create_asvtable, size="tiny"}
seqtab <- makeSequenceTable(dadaFs)
# Dimensions of ASV table
dim(seqtab)
```

#### Remove chimeric variants

```{r chimeras, warning = F, message = F, size = "tiny"}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus",
                                    multithread = T)
# Check new dimensions and amount of data lost
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)

```

Changing parameters results to more variants, but they seem to be mostly chimeric. End result is almost the same, 933 with default parameters vs 969 with less stringent parameters.

#### Summary table

```{r, size="tiny"}
#If processing a single sample, remove the sapply calls
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), rowSums(seqtab.nochim),
rowSums(seqtab.nochim != 0))
colnames(track) <- c("Input", "Filtered", "DenoisedF", "Nonchimeric",
"N:o of variants")
rownames(track) <- rownames(metadata)
# Table
kable(track, caption="Summary table", booktabs = T, longtable = T) %>%
kable_styling(latex_options=c("striped", "HOLD_position", "repeat_header"),
font_size = 11) %>%
row_spec(0,background = "teal", color = "ivory")
```

Number of variants in each samples is also practically unchanged with new parameters.

#### Assign taxonomy

```{r assign_taxonomy, size="tiny", eval=F }
taxonomy <- assignTaxonomy(seqtab.nochim, silva, multithread=3, minBoot=90)
taxonomy <- addSpecies(taxonomy, species, n=300, verbose=F)
saveRDS(taxonomy, "data/denoised/taxonomy.rds")
```

```{r, size="tiny"}
taxonomy <- ("data/denoised/taxonomy.rds")
```

#### Create TSE object

```{r create_tse, size="tiny"}
# Counts table transponed and as matrix
counts <- as.matrix(t(seqtab.nochim))
repseq <- DNAStringSet(rownames(counts))
# Empty rownames
rownames(counts) <- NULL
# New names
ASV_names <- paste0("ASV", seq(nrow(counts)))
# Empty rownames
rownames(taxonomy) <- NULL
# Empty rownames
rownames(metadata) <- NULL
#Create tse, Data in DataFrame format
tse <- TreeSummarizedExperiment(assays = list(counts = counts),
                                rowData = DataFrame(taxonomy),
                                colData = DataFrame(metadata))
# Add names
names(tse) <- ASV_names
colnames(tse) <- sample.names
referenceSeq(tse) <- repseq
#View
tse
```

#### Write results to files

Count table from assays is written to text file

```{r, size="tiny"}
#sample names will be columns
ASVdf <- (data.frame(ASV_names,assays(tse)$counts)) 
#write
write_tsv(ASVdf, paste0(exportloc,"asv_counts.tsv"))
```

Likewise taxonomy table from rowData

```{r,size="tiny"}
#taxonomy ranks in columns
taxadf <- data.frame(ASV_names, rowData(tse))
#write
write_tsv(taxadf,paste0(exportloc,"taxonomy.tsv"))
```

Variant sequences are saved to fasta

```{r, size="tiny"}
tse %>% referenceSeq() %>% 
  writeXStringSet(paste0(exportloc, "repseq.fasta"),
                                      append = F, compress = F,
                                      format = "fasta")
```

Writing also metadata ensures that it is compatible with whole data set

```{r, size="tiny"}
data.frame(colData(tse)) %>% rownames_to_column(var = "Sampleid") %>% write_tsv(paste0(exportloc,"metadata.tsv"))
```

Saving tse as rds file allows easy reading of original object

```{r, size="tiny"}
saveRDS(tse, paste0(exportloc, "tse.rds"))
```
